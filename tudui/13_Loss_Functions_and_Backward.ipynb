{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86a21e2-a056-4199-82a0-2df297891ea3",
   "metadata": {},
   "source": [
    "# 一、什么是Loss Function？   \n",
    "**损失函数** 用于衡量模型预测与真实值之间的差距，是训练过程中的目标函数。  \n",
    "> 损失越小，代表模型的预测越接近真实标签。\n",
    "\n",
    "# 二、常见的损失函数   \n",
    "![常见损失函数](images/Loss_Functions.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "128385c5-660c-409a-97b9-4c8d5db2a103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:05:11.559856Z",
     "start_time": "2025-04-22T11:05:11.556863Z"
    }
   },
   "source": [
    "# torch.nn.L1Loss\n",
    "#     size_average (bool, optional) \n",
    "#     reduce (bool, optional):\n",
    "#     # 上面两者已经过时，推荐使用 reduction 即可\n",
    "#     reduction (str, optional):\n",
    "#         'none': no reduction will be applied, \n",
    "#         'mean': the sum of the output will be divided by the number of elements in the output, \n",
    "#         'sum': the output will be summed.  \n",
    "#         Default: 'mean'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "854bbbee-4da6-4ab3-b661-0172f6296c28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:05:11.974493Z",
     "start_time": "2025-04-22T11:05:11.972642Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from collections import OrderedDict"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "f238a4c9-4beb-420b-9b37-c0ccbf9dd7b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:05:12.433966Z",
     "start_time": "2025-04-22T11:05:12.426Z"
    }
   },
   "source": [
    "# 可以直接在数据中加入一个float\n",
    "inputs = torch.tensor([1.0, 2, 3])\n",
    "# 也可以指定类型\n",
    "targets = torch.tensor([1, 2, 5], dtype = torch.float32)\n",
    "print(inputs.shape)\n",
    "inputs = torch.reshape(inputs, (1, 3))\n",
    "targets = torch.reshape(targets, (1, 3))\n",
    "print(inputs.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "f5366f17-324b-4cc5-b5a6-2ff6e068b6c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:05:37.924167Z",
     "start_time": "2025-04-22T11:05:37.915926Z"
    }
   },
   "source": [
    "loss = nn.L1Loss(reduction = 'sum')\n",
    "result = loss(inputs, targets)\n",
    "print(result)\n",
    "loss = nn.L1Loss(reduction = 'mean')\n",
    "result = loss(inputs, targets)\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(0.6667)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "4606906f-b489-4e2d-9af9-a3600cdea209",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:06:18.325795Z",
     "start_time": "2025-04-22T11:06:18.322173Z"
    }
   },
   "source": [
    "# torch.nn.MSELoss\n",
    "#     size_average (bool, optional)\n",
    "#     reduce (bool, optional)\n",
    "#     # 上面两者已弃用\n",
    "#     reduction (str, optional):\n",
    "#         'none': no reduction will be applied, \n",
    "#         'mean': the sum of the output will be divided by the number of elements in the output, \n",
    "#         'sum': the output will be summed. \n",
    "#         Default: 'mean'"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "cec09d16-2180-4359-bf8f-7bab8bac4ac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:06:25.853309Z",
     "start_time": "2025-04-22T11:06:25.847164Z"
    }
   },
   "source": [
    "loss_MSE = nn.MSELoss(reduction = 'sum') # 对loss进行求和\n",
    "result_MSE = loss_MSE(inputs, targets)\n",
    "print(result_MSE)\n",
    "loss_MSE = nn.MSELoss(reduction = 'mean') # 对loss进行平均\n",
    "result_MSE = loss_MSE(inputs, targets)\n",
    "print(result_MSE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor(1.3333)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "348fedaf-14ea-450e-8488-ff3c1f7f9bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:07:12.259239Z",
     "start_time": "2025-04-22T11:07:12.256125Z"
    }
   },
   "source": [
    "# nn.CrossEntropyLoss 结合了 nn.LogSoftmax 和 nn.NLLLoss, 所以:\n",
    "\t# •输入是 原始 logits（未经过 softmax）；\n",
    "\t# •内部会自动做 log_softmax 处理；\n",
    "\t# •对数似然越大，损失越小。\n",
    "# torch.nn.CrossEntropyLoss(\n",
    "#     weight (Tensor, optional)              类别权重（shape=[C]），用于类别不平衡\n",
    "#     size_average (bool, optional)\n",
    "#     ignore_index (int, optional)           忽略某个类别的标签索引（如在语义分割中忽略边界)\n",
    "#     reduce (bool, optional)\n",
    "#     reduction (str, optional)              同 L1Loss和 MSELoss\n",
    "#     label_smoothing (float, optional)      标签平滑，防止过拟合\n",
    "#         将原本 “硬标签” 的 one-hot 向量，稍微“软化”，变成接近的概率分布，使模型训练更加“宽容”，避免过拟合。\n",
    "#         例如：真实类别是 → [0, 0, 1, 0]，模型如果预测 [0.01, 0.02, 0.96, 0.01]本该有较大的 loss，\n",
    "#              但这已经是一个很好的预测，引入label_smoothing减少这种标签的损失函数，减少过拟合。"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "0ef0818b-c0a9-403f-8da7-aa55e8e77eb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:07:44.392963Z",
     "start_time": "2025-04-22T11:07:44.385549Z"
    }
   },
   "source": [
    "x = torch.tensor([0.1, 0.2, 0.3])\n",
    "y = torch.tensor([1])\n",
    "x = torch.reshape(x, (1, 3))\n",
    "loss_cross = nn.CrossEntropyLoss()\n",
    "result_cross = loss_cross(x, y)\n",
    "print(result_cross)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1019)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "fa7bef6d-f7ef-4803-81ef-e16a81049ca1",
   "metadata": {},
   "source": [
    "# 如何在神经网络中写入 LossFunction"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b049edd-fcb6-4864-a1c0-3bc7d4fba397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:08:17.719535Z",
     "start_time": "2025-04-22T11:08:17.713410Z"
    }
   },
   "source": [
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()\n",
    "        self.model1 = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2)),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=2)),\n",
    "            ('conv2', nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2)),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=2)),\n",
    "            ('conv3', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)),\n",
    "            ('pool3', nn.MaxPool2d(kernel_size=2)),\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(1024, 64)),  # 注意：1024 = 64通道 × 4 × 4（针对输入32x32）\n",
    "            ('fc2', nn.Linear(64, 10))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model1(x)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "74412190-8efd-4c56-93da-4746129726ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:08:34.995411Z",
     "start_time": "2025-04-22T11:08:34.400596Z"
    }
   },
   "source": [
    "dataset = torchvision.datasets.CIFAR10(\"./data/CIFAR10/\", train = False, transform = torchvision.transforms.ToTensor(), download = True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 64)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "f9090c44-32c0-4a5d-9f40-a4418dcc8c76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:10:09.167314Z",
     "start_time": "2025-04-22T11:10:09.060483Z"
    }
   },
   "source": [
    "tudui = Tudui()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "i = 0;\n",
    "for data in dataloader:\n",
    "    if i >= 2:\n",
    "        break\n",
    "    imgs, targets = data\n",
    "    outputs = tudui(imgs)\n",
    "    print(\"output:\")\n",
    "    print(outputs)\n",
    "    print(\"targets:\")\n",
    "    print(targets)\n",
    "    print(\"loss:\")\n",
    "    result_loss = loss(outputs, targets)\n",
    "    print(result_loss)\n",
    "    result_loss.backward()\n",
    "    print(\"ok\")\n",
    "\n",
    "    i = i + 1"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "tensor([[ 0.0037, -0.0074, -0.0213,  0.0122,  0.0981, -0.1133, -0.0756, -0.0018,\n",
      "         -0.0177,  0.0488],\n",
      "        [ 0.0011,  0.0011, -0.0312, -0.0292,  0.1048, -0.1113, -0.0527,  0.0094,\n",
      "         -0.0160,  0.0449],\n",
      "        [-0.0055, -0.0048, -0.0284, -0.0124,  0.1059, -0.1266, -0.0605,  0.0179,\n",
      "         -0.0157,  0.0349],\n",
      "        [-0.0030,  0.0042, -0.0115, -0.0141,  0.1056, -0.1255, -0.0508, -0.0013,\n",
      "          0.0008,  0.0535],\n",
      "        [ 0.0071, -0.0026, -0.0195,  0.0199,  0.0889, -0.0942, -0.0830,  0.0111,\n",
      "          0.0017,  0.0439],\n",
      "        [-0.0078, -0.0203, -0.0189,  0.0148,  0.1055, -0.1099, -0.0792,  0.0149,\n",
      "         -0.0340,  0.0384],\n",
      "        [ 0.0135,  0.0052, -0.0155,  0.0194,  0.1022, -0.0968, -0.0953,  0.0348,\n",
      "         -0.0180,  0.0704],\n",
      "        [ 0.0018, -0.0202, -0.0157,  0.0224,  0.0857, -0.1214, -0.0783,  0.0184,\n",
      "         -0.0290,  0.0355],\n",
      "        [ 0.0144,  0.0103, -0.0162, -0.0010,  0.0882, -0.1063, -0.0605, -0.0020,\n",
      "         -0.0072,  0.0538],\n",
      "        [ 0.0013, -0.0156, -0.0246,  0.0048,  0.0950, -0.1216, -0.0647,  0.0244,\n",
      "         -0.0283,  0.0328],\n",
      "        [ 0.0016,  0.0084, -0.0228,  0.0034,  0.0878, -0.1065, -0.0580, -0.0060,\n",
      "          0.0090,  0.0396],\n",
      "        [-0.0069, -0.0130, -0.0173,  0.0028,  0.1117, -0.1230, -0.0834,  0.0225,\n",
      "         -0.0391,  0.0336],\n",
      "        [ 0.0071, -0.0048, -0.0133,  0.0167,  0.0858, -0.0951, -0.0669,  0.0077,\n",
      "         -0.0069,  0.0556],\n",
      "        [-0.0194,  0.0037,  0.0144, -0.0196,  0.1115, -0.0856, -0.0591,  0.0068,\n",
      "          0.0097,  0.0472],\n",
      "        [-0.0008, -0.0177, -0.0251,  0.0081,  0.0906, -0.1201, -0.0888,  0.0191,\n",
      "         -0.0224,  0.0379],\n",
      "        [-0.0012,  0.0011, -0.0215,  0.0135,  0.0896, -0.1121, -0.0641,  0.0158,\n",
      "         -0.0101,  0.0416],\n",
      "        [ 0.0081,  0.0125, -0.0268, -0.0033,  0.1097, -0.1087, -0.0785,  0.0199,\n",
      "         -0.0111,  0.0585],\n",
      "        [ 0.0235,  0.0008, -0.0266,  0.0199,  0.0944, -0.0933, -0.0743,  0.0055,\n",
      "         -0.0173,  0.0441],\n",
      "        [-0.0191, -0.0034, -0.0209, -0.0324,  0.1154, -0.1218, -0.0594,  0.0169,\n",
      "         -0.0446,  0.0145],\n",
      "        [-0.0004,  0.0076, -0.0024, -0.0059,  0.1090, -0.0918, -0.0768,  0.0060,\n",
      "         -0.0075,  0.0629],\n",
      "        [-0.0130,  0.0002, -0.0373,  0.0087,  0.1009, -0.1172, -0.0978,  0.0204,\n",
      "         -0.0365,  0.0337],\n",
      "        [ 0.0375,  0.0214, -0.0287,  0.0169,  0.1091, -0.0846, -0.0951, -0.0208,\n",
      "          0.0136,  0.0857],\n",
      "        [-0.0134, -0.0189, -0.0299,  0.0198,  0.0808, -0.1213, -0.0873,  0.0080,\n",
      "         -0.0179,  0.0151],\n",
      "        [ 0.0088, -0.0082, -0.0184,  0.0199,  0.0993, -0.1020, -0.1013,  0.0417,\n",
      "         -0.0298,  0.0623],\n",
      "        [ 0.0073,  0.0102, -0.0084,  0.0025,  0.1052, -0.1001, -0.0750,  0.0201,\n",
      "         -0.0219,  0.0414],\n",
      "        [-0.0166, -0.0305, -0.0245,  0.0095,  0.0998, -0.1199, -0.0752,  0.0193,\n",
      "         -0.0396,  0.0165],\n",
      "        [-0.0061, -0.0112, -0.0275,  0.0156,  0.0841, -0.1174, -0.0879,  0.0340,\n",
      "         -0.0326,  0.0422],\n",
      "        [-0.0068, -0.0053, -0.0224,  0.0167,  0.1100, -0.0997, -0.0792,  0.0238,\n",
      "         -0.0222,  0.0424],\n",
      "        [ 0.0025, -0.0112, -0.0106,  0.0175,  0.0943, -0.1069, -0.0783,  0.0257,\n",
      "         -0.0360,  0.0430],\n",
      "        [-0.0078, -0.0189, -0.0191,  0.0263,  0.0999, -0.1147, -0.0998,  0.0425,\n",
      "         -0.0468,  0.0456],\n",
      "        [-0.0035, -0.0269, -0.0195,  0.0285,  0.0937, -0.1162, -0.1040,  0.0354,\n",
      "         -0.0522,  0.0263],\n",
      "        [-0.0023,  0.0072, -0.0203,  0.0003,  0.0978, -0.0988, -0.0820,  0.0159,\n",
      "         -0.0156,  0.0526],\n",
      "        [ 0.0055,  0.0015, -0.0252,  0.0128,  0.0869, -0.1109, -0.0722,  0.0080,\n",
      "         -0.0081,  0.0574],\n",
      "        [ 0.0068, -0.0104, -0.0082,  0.0277,  0.0831, -0.1307, -0.0807,  0.0227,\n",
      "         -0.0326,  0.0504],\n",
      "        [-0.0047, -0.0092, -0.0088,  0.0175,  0.1036, -0.0990, -0.0831,  0.0303,\n",
      "         -0.0330,  0.0290],\n",
      "        [ 0.0127, -0.0234, -0.0212,  0.0111,  0.0915, -0.1205, -0.0877,  0.0054,\n",
      "         -0.0205,  0.0455],\n",
      "        [ 0.0029, -0.0078, -0.0225,  0.0151,  0.1047, -0.0985, -0.0848,  0.0251,\n",
      "         -0.0243,  0.0502],\n",
      "        [ 0.0086, -0.0011, -0.0105, -0.0109,  0.1129, -0.1144, -0.0598,  0.0104,\n",
      "         -0.0097,  0.0583],\n",
      "        [ 0.0039, -0.0029, -0.0267,  0.0127,  0.1035, -0.1116, -0.0782,  0.0312,\n",
      "         -0.0265,  0.0553],\n",
      "        [ 0.0266,  0.0187, -0.0282,  0.0024,  0.0977, -0.0932, -0.0664,  0.0076,\n",
      "          0.0106,  0.0578],\n",
      "        [ 0.0154,  0.0037, -0.0115,  0.0127,  0.0961, -0.1100, -0.0617, -0.0018,\n",
      "         -0.0119,  0.0362],\n",
      "        [-0.0023, -0.0253, -0.0094,  0.0284,  0.0954, -0.1124, -0.0975,  0.0312,\n",
      "         -0.0529,  0.0235],\n",
      "        [-0.0019, -0.0179, -0.0352,  0.0347,  0.1042, -0.1149, -0.0885,  0.0231,\n",
      "         -0.0239,  0.0331],\n",
      "        [ 0.0056, -0.0142, -0.0197,  0.0303,  0.0819, -0.1227, -0.0868,  0.0223,\n",
      "         -0.0387,  0.0353],\n",
      "        [-0.0122, -0.0283, -0.0107, -0.0004,  0.0891, -0.1196, -0.0542,  0.0062,\n",
      "         -0.0374, -0.0049],\n",
      "        [ 0.0042, -0.0160, -0.0358, -0.0008,  0.1058, -0.1101, -0.0620, -0.0023,\n",
      "          0.0038,  0.0462],\n",
      "        [ 0.0027,  0.0029, -0.0118, -0.0002,  0.1017, -0.0967, -0.0878,  0.0190,\n",
      "         -0.0083,  0.0623],\n",
      "        [-0.0014, -0.0105, -0.0291,  0.0072,  0.1007, -0.1020, -0.0749,  0.0196,\n",
      "         -0.0057,  0.0326],\n",
      "        [ 0.0035, -0.0063, -0.0060,  0.0132,  0.0975, -0.1057, -0.0813,  0.0155,\n",
      "         -0.0264,  0.0453],\n",
      "        [-0.0088, -0.0123, -0.0133,  0.0284,  0.0945, -0.1071, -0.0889,  0.0304,\n",
      "         -0.0313,  0.0373],\n",
      "        [-0.0011, -0.0386, -0.0108,  0.0140,  0.0958, -0.1180, -0.0740,  0.0210,\n",
      "         -0.0399,  0.0198],\n",
      "        [-0.0120, -0.0270, -0.0193,  0.0181,  0.0920, -0.1230, -0.0800,  0.0131,\n",
      "         -0.0330,  0.0259],\n",
      "        [ 0.0046, -0.0051, -0.0117,  0.0126,  0.0913, -0.1057, -0.0839,  0.0256,\n",
      "         -0.0209,  0.0536],\n",
      "        [-0.0072, -0.0102, -0.0153, -0.0035,  0.1114, -0.1074, -0.0787,  0.0439,\n",
      "         -0.0325,  0.0403],\n",
      "        [ 0.0046, -0.0077, -0.0306, -0.0019,  0.1120, -0.1168, -0.0606,  0.0082,\n",
      "         -0.0239,  0.0322],\n",
      "        [-0.0064, -0.0067, -0.0238, -0.0033,  0.0963, -0.1139, -0.0610,  0.0080,\n",
      "         -0.0167,  0.0325],\n",
      "        [ 0.0224,  0.0087, -0.0341,  0.0202,  0.1034, -0.0935, -0.0769, -0.0086,\n",
      "         -0.0097,  0.0451],\n",
      "        [-0.0018, -0.0070, -0.0128,  0.0063,  0.0929, -0.1098, -0.0832,  0.0145,\n",
      "         -0.0317,  0.0443],\n",
      "        [ 0.0021, -0.0080, -0.0226, -0.0037,  0.0964, -0.1074, -0.0672,  0.0093,\n",
      "         -0.0127,  0.0426],\n",
      "        [ 0.0155,  0.0066, -0.0234,  0.0262,  0.1083, -0.0958, -0.0793,  0.0076,\n",
      "         -0.0103,  0.0531],\n",
      "        [ 0.0083, -0.0053, -0.0193,  0.0304,  0.0960, -0.0860, -0.0724,  0.0153,\n",
      "         -0.0143,  0.0571],\n",
      "        [ 0.0053,  0.0003, -0.0140,  0.0219,  0.1053, -0.0894, -0.0935,  0.0206,\n",
      "         -0.0124,  0.0622],\n",
      "        [-0.0129, -0.0154, -0.0059,  0.0185,  0.1019, -0.1088, -0.0788,  0.0092,\n",
      "         -0.0147,  0.0378],\n",
      "        [ 0.0057, -0.0033, -0.0209,  0.0125,  0.1259, -0.0871, -0.0890,  0.0259,\n",
      "         -0.0526,  0.0305]], grad_fn=<AddmmBackward0>)\n",
      "targets:\n",
      "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
      "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
      "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3])\n",
      "loss:\n",
      "tensor(2.3114, grad_fn=<NllLossBackward0>)\n",
      "ok\n",
      "output:\n",
      "tensor([[ 3.3465e-03, -2.5383e-02, -1.7107e-02,  3.1378e-02,  8.6306e-02,\n",
      "         -1.1002e-01, -7.5043e-02,  1.6194e-02, -3.3330e-02,  3.3828e-02],\n",
      "        [ 3.9633e-03, -1.1111e-02, -2.3893e-02,  3.3248e-02,  9.1653e-02,\n",
      "         -1.1522e-01, -8.7318e-02,  1.3793e-02, -2.8209e-02,  2.5647e-02],\n",
      "        [-6.5692e-03, -2.4807e-02, -2.1312e-02,  3.2416e-03,  9.6095e-02,\n",
      "         -1.2064e-01, -7.7466e-02, -5.2971e-04, -1.6804e-02,  3.8998e-02],\n",
      "        [ 6.4820e-03, -1.1289e-02, -4.2169e-02,  1.0908e-02,  8.0975e-02,\n",
      "         -1.1673e-01, -7.2296e-02, -2.7490e-03,  1.9409e-02,  5.0937e-02],\n",
      "        [-6.9137e-03, -2.2069e-02, -3.3826e-02,  6.4570e-03,  1.0240e-01,\n",
      "         -1.1551e-01, -9.2411e-02,  2.9591e-02, -3.0639e-02,  2.9380e-02],\n",
      "        [-1.9040e-03, -3.1315e-02, -1.7878e-02, -7.8898e-03,  1.1043e-01,\n",
      "         -1.2233e-01, -7.5313e-02,  2.7567e-02, -3.4123e-02,  2.2003e-02],\n",
      "        [ 2.5513e-03, -1.6843e-03, -3.1007e-02, -9.1838e-04,  1.0305e-01,\n",
      "         -1.0991e-01, -6.0590e-02, -1.9044e-04, -1.8007e-02,  4.1357e-02],\n",
      "        [ 2.8120e-03, -1.9995e-02, -1.4527e-02,  1.7599e-02,  9.4840e-02,\n",
      "         -1.0977e-01, -8.0101e-02,  5.2902e-03, -2.4661e-02,  3.6979e-02],\n",
      "        [-3.3296e-03, -4.5386e-03, -1.6999e-02, -4.1965e-03,  1.0724e-01,\n",
      "         -1.0505e-01, -5.6282e-02,  1.6277e-03, -9.0237e-03,  4.9811e-02],\n",
      "        [-8.2344e-03,  8.3938e-04, -2.5196e-02, -4.0584e-03,  8.9075e-02,\n",
      "         -1.1511e-01, -6.5386e-02,  5.4173e-03, -1.4900e-02,  2.9803e-02],\n",
      "        [-1.2101e-02, -1.6111e-02, -2.9004e-02, -5.7518e-03,  1.0850e-01,\n",
      "         -1.2958e-01, -6.5817e-02,  2.1231e-02, -5.0594e-02,  1.2343e-02],\n",
      "        [ 8.6287e-03,  6.3090e-03, -1.6089e-03,  2.7491e-02,  8.9740e-02,\n",
      "         -1.0504e-01, -7.9076e-02,  2.0331e-02, -1.7388e-02,  4.7279e-02],\n",
      "        [ 1.4408e-03, -2.0862e-02, -2.8500e-02,  1.2037e-02,  1.1197e-01,\n",
      "         -9.6990e-02, -7.4946e-02,  2.4048e-03, -1.5959e-02,  4.6684e-02],\n",
      "        [-1.2059e-03,  8.6286e-03, -1.6484e-02, -3.2390e-03,  1.0289e-01,\n",
      "         -8.2614e-02, -7.5677e-02,  1.1564e-02,  1.2004e-02,  6.5990e-02],\n",
      "        [-5.0793e-03,  4.2059e-03,  4.0390e-04,  4.3834e-03,  9.9681e-02,\n",
      "         -8.9964e-02, -8.6441e-02,  3.2127e-02, -6.0511e-03,  5.8577e-02],\n",
      "        [ 2.1005e-02,  1.9151e-02, -2.0468e-02, -1.4121e-02,  1.0930e-01,\n",
      "         -9.4187e-02, -3.7541e-02, -5.1522e-03, -8.2426e-03,  5.4853e-02],\n",
      "        [ 3.2089e-03,  1.6788e-03, -3.5232e-02, -9.7394e-03,  1.0227e-01,\n",
      "         -1.1492e-01, -6.3628e-02,  1.0056e-02, -1.7764e-02,  4.1653e-02],\n",
      "        [ 7.7273e-03, -1.2343e-02, -4.2928e-03,  1.2965e-02,  9.8904e-02,\n",
      "         -9.3945e-02, -8.8169e-02,  3.4411e-02, -1.6743e-02,  6.5284e-02],\n",
      "        [ 9.4499e-03, -3.6524e-03, -1.7866e-02,  1.6791e-02,  8.9409e-02,\n",
      "         -1.0318e-01, -9.3447e-02,  1.1212e-02, -1.3773e-02,  5.1450e-02],\n",
      "        [ 2.1008e-02,  2.3794e-02, -9.9902e-03, -1.3920e-02,  1.0741e-01,\n",
      "         -9.8874e-02, -6.3381e-02, -4.9843e-03,  8.0006e-03,  6.2416e-02],\n",
      "        [ 2.3042e-02, -6.9152e-03, -2.0526e-02, -1.1000e-02,  9.8113e-02,\n",
      "         -1.1549e-01, -5.6083e-02, -2.1747e-03, -1.7858e-02,  4.7699e-02],\n",
      "        [-1.1800e-02, -2.6823e-02, -3.1520e-02,  1.8795e-02,  1.0583e-01,\n",
      "         -1.1788e-01, -9.3994e-02,  3.7827e-02, -4.1667e-02,  1.5928e-02],\n",
      "        [ 7.0761e-03, -8.4632e-03, -2.8815e-02,  1.1777e-02,  1.0289e-01,\n",
      "         -1.2313e-01, -8.0230e-02,  2.2622e-03, -2.0375e-02,  2.0164e-02],\n",
      "        [ 7.0311e-03,  7.4903e-03, -1.0613e-02, -3.1472e-02,  1.1057e-01,\n",
      "         -9.7780e-02, -4.5111e-02,  7.2133e-03, -5.2487e-03,  4.9438e-02],\n",
      "        [ 1.0134e-02,  1.0733e-02, -2.2530e-02, -1.3993e-02,  1.0444e-01,\n",
      "         -1.1114e-01, -4.4108e-02,  1.1865e-02, -3.1110e-02,  3.6554e-02],\n",
      "        [ 1.0618e-03, -6.5139e-03, -2.2627e-02, -8.0076e-03,  9.7568e-02,\n",
      "         -1.0386e-01, -6.3838e-02,  7.1936e-03, -1.8039e-02,  3.3850e-02],\n",
      "        [ 5.2299e-03, -4.8025e-03, -2.6994e-02,  1.5595e-02,  8.2720e-02,\n",
      "         -1.0538e-01, -7.0673e-02, -5.4774e-04,  2.8869e-03,  5.1473e-02],\n",
      "        [ 1.3860e-03, -1.5686e-02, -1.9191e-02,  1.5837e-02,  9.5251e-02,\n",
      "         -1.0949e-01, -1.0002e-01,  2.3587e-02, -2.7704e-02,  4.9119e-02],\n",
      "        [ 4.5295e-03, -1.1171e-02, -2.4991e-02, -1.8837e-02,  9.2054e-02,\n",
      "         -1.0015e-01, -3.8663e-02,  6.0401e-03, -5.0241e-03,  4.4453e-02],\n",
      "        [-1.4388e-03, -1.9604e-02, -1.6408e-02,  2.7190e-02,  1.0097e-01,\n",
      "         -1.0943e-01, -8.9692e-02,  2.6443e-02, -4.0981e-02,  3.4369e-02],\n",
      "        [-1.1773e-02, -1.7434e-02, -1.9422e-02,  1.3818e-02,  9.3864e-02,\n",
      "         -1.1233e-01, -8.5635e-02,  2.3191e-02, -3.1464e-02,  2.0367e-02],\n",
      "        [ 6.2283e-03, -8.4612e-03, -2.4956e-02,  1.8295e-02,  1.1009e-01,\n",
      "         -9.3919e-02, -7.6998e-02,  2.3827e-02, -8.5597e-03,  5.8692e-02],\n",
      "        [ 4.0315e-04, -1.4995e-02, -2.5987e-02,  2.7864e-02,  9.9178e-02,\n",
      "         -1.1757e-01, -8.9782e-02,  1.6903e-02, -2.0695e-02,  4.9090e-02],\n",
      "        [-9.0855e-04,  2.5972e-03, -3.2847e-02,  2.4554e-03,  8.9676e-02,\n",
      "         -1.0852e-01, -6.9342e-02,  1.2959e-03, -2.1000e-03,  3.9038e-02],\n",
      "        [ 4.2333e-02,  1.5582e-02, -2.7682e-02,  2.2038e-02,  1.1741e-01,\n",
      "         -8.7265e-02, -8.6909e-02,  3.2470e-03,  1.6887e-02,  9.1095e-02],\n",
      "        [ 1.3102e-02,  2.5016e-02, -1.2559e-02, -1.3795e-02,  1.1200e-01,\n",
      "         -9.0775e-02, -7.0151e-02,  2.4896e-02, -1.0251e-02,  6.7470e-02],\n",
      "        [ 4.7475e-03, -1.1613e-02, -3.2151e-02,  2.7090e-02,  8.9060e-02,\n",
      "         -1.0346e-01, -7.6667e-02,  3.2183e-03, -7.0702e-03,  4.4843e-02],\n",
      "        [ 1.2812e-02, -6.7409e-03, -2.0744e-02,  1.0991e-02,  8.6186e-02,\n",
      "         -1.1033e-01, -6.7367e-02, -2.3795e-03, -8.3726e-03,  4.8143e-02],\n",
      "        [-1.3465e-02, -9.4576e-04, -3.5150e-03, -1.8127e-03,  1.0585e-01,\n",
      "         -8.7904e-02, -7.8366e-02,  2.0457e-02, -1.8779e-02,  4.7302e-02],\n",
      "        [ 6.4540e-03, -3.0662e-02, -1.3529e-02,  2.7052e-02,  1.0690e-01,\n",
      "         -1.1410e-01, -5.1567e-02,  1.9230e-02, -4.0278e-02,  1.4946e-02],\n",
      "        [ 8.2463e-03, -1.1492e-02, -1.5278e-03,  2.3432e-02,  9.5284e-02,\n",
      "         -1.0168e-01, -6.7619e-02, -2.2310e-03,  6.6432e-03,  4.9751e-02],\n",
      "        [ 2.3195e-02, -5.5187e-03, -2.5471e-02, -2.8194e-02,  1.1282e-01,\n",
      "         -1.2113e-01, -4.7794e-02, -3.1485e-03, -2.6392e-02,  3.6501e-02],\n",
      "        [ 2.0388e-03, -1.4264e-02, -2.9042e-02,  1.3114e-02,  8.7971e-02,\n",
      "         -1.1757e-01, -8.7398e-02,  6.7106e-03, -1.6469e-02,  4.7327e-02],\n",
      "        [ 1.1086e-03, -1.9500e-02, -1.7329e-02,  2.1059e-02,  9.3139e-02,\n",
      "         -1.1401e-01, -9.4569e-02,  3.8113e-02, -3.0495e-02,  4.0667e-02],\n",
      "        [ 2.3114e-02,  1.6203e-02, -1.1708e-02, -1.6946e-02,  9.0703e-02,\n",
      "         -1.0968e-01, -4.9648e-02, -4.9637e-03,  1.0464e-02,  6.0830e-02],\n",
      "        [ 3.5264e-02,  1.6069e-02, -2.2415e-02, -1.7557e-02,  1.0487e-01,\n",
      "         -1.0739e-01, -7.3543e-02, -1.3650e-02,  7.9264e-03,  5.6900e-02],\n",
      "        [ 9.1062e-03, -1.0230e-02, -1.3366e-02,  1.0067e-02,  1.2237e-01,\n",
      "         -9.8735e-02, -8.4176e-02,  4.5776e-02, -2.5528e-02,  7.1934e-02],\n",
      "        [ 3.5906e-02, -6.1834e-03, -3.2783e-02,  2.6577e-02,  8.9937e-02,\n",
      "         -8.7320e-02, -7.4757e-02,  1.2236e-02,  1.2087e-02,  7.1141e-02],\n",
      "        [ 4.0412e-03, -1.2551e-02, -2.0851e-02,  7.6444e-03,  1.1215e-01,\n",
      "         -1.0287e-01, -8.0565e-02,  2.2279e-02, -3.5526e-02,  4.6774e-02],\n",
      "        [-8.6024e-06, -1.8328e-02, -2.6451e-02,  4.9689e-03,  9.5937e-02,\n",
      "         -1.1140e-01, -7.6209e-02,  1.8015e-02, -2.5643e-02,  4.1873e-02],\n",
      "        [-9.2699e-03, -1.2077e-02, -2.6316e-02,  2.7806e-04,  1.2233e-01,\n",
      "         -1.0177e-01, -7.3860e-02,  3.7172e-02, -3.1953e-02,  5.3428e-02],\n",
      "        [-4.1690e-03, -2.6899e-02, -3.8350e-02,  1.3689e-02,  1.0327e-01,\n",
      "         -1.1827e-01, -9.2204e-02,  8.9586e-03, -4.3946e-02,  1.6336e-02],\n",
      "        [ 4.9817e-03,  4.0833e-03, -2.7817e-02,  1.1428e-02,  9.5808e-02,\n",
      "         -1.0762e-01, -8.4653e-02, -4.4486e-03,  8.5699e-03,  5.0698e-02],\n",
      "        [-7.0705e-03, -2.1233e-02, -1.9974e-02,  2.6426e-04,  1.0945e-01,\n",
      "         -1.0848e-01, -9.7644e-02,  2.7957e-02, -2.6012e-02,  4.0271e-02],\n",
      "        [-1.3456e-02, -3.0895e-02, -1.7837e-02,  2.9809e-02,  9.1782e-02,\n",
      "         -1.1693e-01, -1.0910e-01,  2.7389e-02, -4.5379e-02,  1.7524e-02],\n",
      "        [ 3.3282e-03,  1.6244e-02, -6.2796e-03,  6.6721e-03,  1.0422e-01,\n",
      "         -8.2484e-02, -8.1880e-02,  1.5398e-02, -6.9887e-03,  7.1017e-02],\n",
      "        [-8.0338e-03, -7.5986e-03, -2.1100e-02, -1.3579e-02,  1.0878e-01,\n",
      "         -1.0914e-01, -6.5973e-02,  9.9893e-03, -1.7763e-02,  4.3761e-02],\n",
      "        [ 7.0223e-03, -1.7392e-02, -1.3215e-02,  4.3584e-02,  8.2007e-02,\n",
      "         -1.0679e-01, -1.0519e-01,  1.7242e-02, -1.6241e-02,  5.0937e-02],\n",
      "        [ 6.5478e-05, -8.1620e-03, -1.6960e-02,  9.0503e-03,  9.7970e-02,\n",
      "         -1.1517e-01, -8.6835e-02,  2.8665e-02, -2.6189e-02,  4.0334e-02],\n",
      "        [ 1.1574e-03,  1.3583e-02, -1.7431e-02,  2.1864e-02,  1.0295e-01,\n",
      "         -9.6809e-02, -9.3533e-02,  3.8683e-02, -2.7209e-02,  5.7064e-02],\n",
      "        [-1.3743e-03, -2.5835e-03, -3.8285e-02, -6.7977e-04,  1.0419e-01,\n",
      "         -1.0673e-01, -6.8554e-02, -1.5534e-04, -2.8406e-02,  2.3562e-02],\n",
      "        [-4.8980e-04, -4.3848e-03, -1.7143e-02,  1.3678e-02,  8.7545e-02,\n",
      "         -1.1543e-01, -7.5453e-02,  2.5857e-02, -3.1257e-02,  4.0373e-02],\n",
      "        [ 5.0419e-03, -1.6450e-02, -2.5921e-02,  1.1725e-02,  1.0002e-01,\n",
      "         -1.1189e-01, -7.4953e-02,  1.7692e-02, -2.4750e-02,  3.4306e-02],\n",
      "        [ 4.2659e-03,  2.6355e-03, -1.6941e-02, -4.2700e-04,  1.0572e-01,\n",
      "         -9.7114e-02, -7.5498e-02,  1.5750e-02, -8.7647e-03,  4.4421e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "targets:\n",
      "tensor([6, 2, 1, 2, 3, 7, 2, 6, 8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7,\n",
      "        8, 9, 0, 3, 8, 6, 4, 6, 6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0,\n",
      "        6, 2, 1, 3, 0, 4, 2, 7, 8, 3, 1, 2, 8, 0, 8, 3])\n",
      "loss:\n",
      "tensor(2.3074, grad_fn=<NllLossBackward0>)\n",
      "ok\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "3b5e46bb-1146-48ab-bb31-3be3c6e58850",
   "metadata": {},
   "source": [
    "# backward() 是 PyTorch 中自动求导（autograd）机制的核心方法，用于执行反向传播（backpropagation），计算所有需要梯度的张量的 .grad 值。\n",
    "# 一般配合优化器来用，详见下一节"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
