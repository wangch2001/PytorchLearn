{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 模型训练完整流程",
   "id": "f1d07d678ae051dd"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-24T04:08:54.015759Z",
     "start_time": "2025-04-24T04:08:54.012091Z"
    }
   },
   "source": [
    "# 介绍一个函数，在分类问题中常用\n",
    "# torch.argmax\n",
    "#     input,                   输入张量\n",
    "#     dim=None,                指定沿哪个维度找最大值的位置（默认：展平成1维后求最大值索引）\n",
    "#     keepdim=False            是否保持原始维度（True返回shape中保留该dim维)\n",
    "\n",
    "# 示例：\n",
    "    # x = torch.tensor([[1, 5, 2],\n",
    "    #                   [7, 3, 9]])\n",
    "    # # 按行取最大值的索引（dim=1）\n",
    "    # row_max_idx = torch.argmax(x, dim=1)\n",
    "    # print(row_max_idx)  # tensor([1, 2]) -> 每行最大值索引位置\n",
    "\n",
    "    # # 按列取最大值的索引（dim=0）\n",
    "    # col_max_idx = torch.argmax(x, dim=0)\n",
    "    # print(col_max_idx)  # tensor([1, 0, 1]) -> 每列最大值索引位置"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:08:55.205412Z",
     "start_time": "2025-04-24T04:08:54.464730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "from Model_save import LeNet_5"
   ],
   "id": "f43130f362f5b8d4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:08:57.517016Z",
     "start_time": "2025-04-24T04:08:56.304754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10(\"./data/CIFAR10\", train=True, download=True,\n",
    "                                           transform=torchvision.transforms.ToTensor())\n",
    "val_data = torchvision.datasets.CIFAR10(\"./data/CIFAR10\", train = False, download=True, transform = torchvision.transforms.ToTensor())\n",
    "\n",
    "# 2. 查看数据集大小\n",
    "train_data_size = len(train_data)\n",
    "val_data_size = len(val_data)\n",
    "print(\"训练数据集长度为：{}\".format(train_data_size))\n",
    "print(\"测试数据集长度为：{}\".format(val_data_size))\n",
    "\n",
    "# 3. 创建数据加载器\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=64)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=64)"
   ],
   "id": "7f383d103f9bb2e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "训练数据集长度为：50000\n",
      "测试数据集长度为：10000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:09:00.219975Z",
     "start_time": "2025-04-24T04:09:00.210521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. 创建模型，网络见Model_save.py\n",
    "model = LeNet_5()\n",
    "\n",
    "# 5. 定义损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6. 定义优化器\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ],
   "id": "b476c95d7b6cef9c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:09:02.917029Z",
     "start_time": "2025-04-24T04:09:02.911804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置训练网络的一些参数\n",
    "#记录训练的次数\n",
    "total_train_step = 0\n",
    "#记录测试的次数\n",
    "total_test_step = 0\n",
    "# 训练的轮数\n",
    "epoch = 10\n",
    "#绘制训练图像\n",
    "writer = SummaryWriter(\"./logs/17_Model_training\")"
   ],
   "id": "5f95da630a0c5c5d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:13:06.703367Z",
     "start_time": "2025-04-24T04:11:45.958996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7. 训练模型\n",
    "for i in range(epoch):\n",
    "    print(\"---------------------第{}轮训练开始---------------------\".format(i + 1))\n",
    "\n",
    "    # 训练步骤\n",
    "    model.train()\n",
    "    for data in train_dataloader:\n",
    "        img, target = data\n",
    "        output = model(img)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # 优化器梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 查看训练结果\n",
    "        total_train_step += 1\n",
    "        if total_train_step % 100 == 0:\n",
    "            print(\"训练次数：{}, loss：{}\".format(total_train_step, loss.item()))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    # 每一轮训练结束之后在测试集上验证模型结果，对模型进行评估，在测试集上不对魔心进行调优\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        right_sum = 0\n",
    "        for data in val_dataloader:\n",
    "            imgs, targets = data\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_test_loss += loss.item()\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            right_sum += torch.sum(outputs == targets).sum()\n",
    "        print(\"整体测试集上的loss：{}\".format(total_test_loss))\n",
    "        writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
    "        print(\"整体测试集上的正确率：{}\".format(right_sum / 10000))\n",
    "        writer.add_scalar(\"test_accuracy\", right_sum / 10000, total_test_step)\n",
    "        total_test_step += 1\n",
    "\n",
    "    # 8. 保存模型\n",
    "    # 方式一：\n",
    "    # torch.save(model, \"./models/LeNet_5/LeNet_5_No_{}_loss_{}.pth\".format(epoch, total_test_step))\n",
    "    # 方式二：（推荐）\n",
    "    torch.save(model.state_dict(), \"./models/LeNet_5/LeNet_5_No_{}_loss_{}.pth\".format(epoch, total_test_step))\n",
    "    print(\"模型已保存\")\n",
    "\n",
    "writer.close()\n"
   ],
   "id": "708fc0361c460405",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------第1轮训练开始---------------------\n",
      "训练次数：800, loss：1.8553754091262817\n",
      "训练次数：900, loss：1.8357659578323364\n",
      "训练次数：1000, loss：1.9563456773757935\n",
      "训练次数：1100, loss：1.960581660270691\n",
      "训练次数：1200, loss：1.707315444946289\n",
      "训练次数：1300, loss：1.671413779258728\n",
      "训练次数：1400, loss：1.7597821950912476\n",
      "训练次数：1500, loss：1.801198959350586\n",
      "整体测试集上的loss：295.43255150318146\n",
      "整体测试集上的正确率：0.32359999418258667\n",
      "模型已保存\n",
      "---------------------第2轮训练开始---------------------\n",
      "训练次数：1600, loss：1.721817135810852\n",
      "训练次数：1700, loss：1.6426852941513062\n",
      "训练次数：1800, loss：1.9466661214828491\n",
      "训练次数：1900, loss：1.7322067022323608\n",
      "训练次数：2000, loss：1.9288573265075684\n",
      "训练次数：2100, loss：1.5295467376708984\n",
      "训练次数：2200, loss：1.455535888671875\n",
      "训练次数：2300, loss：1.7748297452926636\n",
      "整体测试集上的loss：270.7079870700836\n",
      "整体测试集上的正确率：0.3716999888420105\n",
      "模型已保存\n",
      "---------------------第3轮训练开始---------------------\n",
      "训练次数：2400, loss：1.700143814086914\n",
      "训练次数：2500, loss：1.3359973430633545\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# 反向传播\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# 更新参数\u001B[39;00m\n\u001B[1;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    487\u001B[0m     )\n\u001B[0;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f603898e7575aa11"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
